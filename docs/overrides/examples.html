{% extends "main.html" %}

{% block content %}
<section class="tx-container">
    <div class="md-grid md-typeset">
        <div class="-landing__highlights">
            <div class="tx-landing__highlights_text">
                <h2>Examples</h2>
            </div>

            <div class="tx-landing__highlights_grid">
                <a href="/examples/llama-index-weaviate">
                    <div class="feature-cell">
                        <div class="feature-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M12 3C7.58 3 4 4.79 4 7s3.58 4 8 4 8-1.79 8-4-3.58-4-8-4M4 9v3c0 2.21 3.58 4 8 4s8-1.79 8-4V9c0 2.21-3.58 4-8 4s-8-1.79-8-4m0 5v3c0 2.21 3.58 4 8 4s8-1.79 8-4v-3c0 2.21-3.58 4-8 4s-8-1.79-8-4Z"></path>
                            </svg>
                        </div>
                        <h3>
                            RAG with Llama Index and Weaviate
                        </h3>

                        <p>
                            Use <strong>Llama Index</strong> and <strong>Weaviate</strong> to enhance the capabilities
                            of LLMs with the context of your data.
                        </p>
                    </div>
                </a>

                <a href="/examples/deploy-python">
                    <div class="feature-cell">
                        <div class="feature-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M7 7H5a2 2 0 0 0-2 2v8h2v-4h2v4h2V9a2 2 0 0 0-2-2m0 4H5V9h2m7-2h-4v10h2v-4h2a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2m0 4h-2V9h2m6 0v6h1v2h-4v-2h1V9h-1V7h4v2Z"></path>
                            </svg>
                        </div>
                        <h3>
                            Deploying LLMs using Python API
                        </h3>

                        <p>
                            <strong>Streamlit</strong> application that programmatically deploys a
                            <strong>Llama</strong> LLM using <strong>dstack</strong>'s <strong>Python API</strong>.
                        </p>
                    </div>
                </a>

                <a href="/examples/finetuning-llama-2">
                    <div class="feature-cell">
                        <div class="feature-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M3 17v2h6v-2H3M3 5v2h10V5H3m10 16v-2h8v-2h-8v-2h-2v6h2M7 9v2H3v2h4v2h2V9H7m14 4v-2H11v2h10m-6-4h2V7h4V5h-4V3h-2v6Z"></path>
                            </svg>
                        </div>
                        <h3>
                            Fine-tuning Llama 2 using QLoRA
                        </h3>

                        <p>
                            Fine-tune <strong>Llama 2</strong> on a custom dataset using the <strong>peft</strong>, and
                            <strong>bitsandbytes</strong>, and <strong>trl</strong> libraries.
                        </p>
                    </div>
                </a>

                <a href="/examples/text-generation-inference">
                    <div class="feature-cell">
                        <div class="feature-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M16 9h3l-5 7m-4-7h4l-2 8M5 9h3l2 7m5-12h2l2 3h-3m-5-3h2l1 3h-4M7 4h2L8 7H5m1-5L2 8l10 14L22 8l-4-6H6Z"></path>
                            </svg>
                        </div>
                        <h3>
                            Deploying LLMs using TGI
                            <br/><br/>
                        </h3>

                        <p>
                            Deploy LLMs as services with optimized performance using <strong>TGI</strong>, an
                            open-source serving framework by Hugging Face.
                        </p>
                    </div>
                </a>

                <a href="/examples/stable-diffusion-xl">
                    <div class="feature-cell">
                        <div class="feature-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M17.5 12a1.5 1.5 0 0 1-1.5-1.5A1.5 1.5 0 0 1 17.5 9a1.5 1.5 0 0 1 1.5 1.5 1.5 1.5 0 0 1-1.5 1.5m-3-4A1.5 1.5 0 0 1 13 6.5 1.5 1.5 0 0 1 14.5 5 1.5 1.5 0 0 1 16 6.5 1.5 1.5 0 0 1 14.5 8m-5 0A1.5 1.5 0 0 1 8 6.5 1.5 1.5 0 0 1 9.5 5 1.5 1.5 0 0 1 11 6.5 1.5 1.5 0 0 1 9.5 8m-3 4A1.5 1.5 0 0 1 5 10.5 1.5 1.5 0 0 1 6.5 9 1.5 1.5 0 0 1 8 10.5 1.5 1.5 0 0 1 6.5 12M12 3a9 9 0 0 0-9 9 9 9 0 0 0 9 9 1.5 1.5 0 0 0 1.5-1.5c0-.39-.15-.74-.39-1-.23-.27-.38-.62-.38-1a1.5 1.5 0 0 1 1.5-1.5H16a5 5 0 0 0 5-5c0-4.42-4.03-8-9-8Z"></path>
                            </svg>
                        </div>
                        <h3>
                            Deploying Stable Diffusion using FastAPI
                        </h3>

                        <p>
                            Deploy <strong>Stable Diffusion XL</strong> as a service using <strong>FastAPI</strong>.
                            <br/><br/>
                        </p>
                    </div>
                </a>

                <a href="/examples/vllm">
                    <div class="feature-cell">
                        <div class="feature-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="-3 -3 27 27">
                                <path d="m13.13 22.19-1.63-3.83c1.57-.58 3.04-1.36 4.4-2.27l-2.77 6.1M5.64 12.5l-3.83-1.63 6.1-2.77C7 9.46 6.22 10.93 5.64 12.5M21.61 2.39S16.66.269 11 5.93c-2.19 2.19-3.5 4.6-4.35 6.71-.28.75-.09 1.57.46 2.13l2.13 2.12c.55.56 1.37.74 2.12.46A19.1 19.1 0 0 0 18.07 13c5.66-5.66 3.54-10.61 3.54-10.61m-7.07 7.07c-.78-.78-.78-2.05 0-2.83s2.05-.78 2.83 0c.77.78.78 2.05 0 2.83-.78.78-2.05.78-2.83 0m-5.66 7.07-1.41-1.41 1.41 1.41M6.24 22l3.64-3.64c-.34-.09-.67-.24-.97-.45L4.83 22h1.41M2 22h1.41l4.77-4.76-1.42-1.41L2 20.59V22m0-2.83 4.09-4.08c-.21-.3-.36-.62-.45-.97L2 17.76v1.41Z"></path>
                            </svg>
                        </div>
                        <h3>
                            Deploying LLMs using vLLM
                            <br/><br/>
                        </h3>

                        <p>
                            Deploy LLMs as services with up to 24 times higher throughput using
                            the
                            <strong>vLLM</strong> library.
                        </p>
                    </div>
                </a>
            </div>
        </div>
    </div>
</section>
<br>
<br>
<br>
<br>
{% endblock %}