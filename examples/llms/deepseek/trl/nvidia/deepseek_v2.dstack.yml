type: task
# The name is optional, if not specified, generated randomly
name: trl-train-deepseek-v2

python: "3.10"

nvcc: true
# Required environment variables
env:
  - WANDB_API_KEY
  - WANDB_PROJECT
  - MODEL_ID=deepseek-ai/DeepSeek-V2-Lite
  - ACCELERATE_USE_FSDP=False
# Commands of the task
commands:
  - git clone https://github.com/huggingface/peft.git
  - pip install trl
  - pip install peft
  - pip install wandb
  - pip install bitsandbytes
  - cd peft/examples/sft
  - python train.py
    --seed 100
    --model_name_or_path "deepseek-ai/DeepSeek-V2-Lite"
    --dataset_name "smangrul/ultrachat-10k-chatml"
    --chat_template_format "chatml"
    --add_special_tokens False
    --append_concat_token False
    --splits "train,test"
    --max_seq_len 512
    --num_train_epochs 1
    --logging_steps 5
    --log_level "info"
    --logging_strategy "steps"
    --eval_strategy "epoch"
    --save_strategy "epoch"
    --hub_private_repo True
    --hub_strategy "every_save"
    --bf16 True
    --packing True
    --learning_rate 1e-4
    --lr_scheduler_type "cosine"
    --weight_decay 1e-4
    --warmup_ratio 0.0
    --max_grad_norm 1.0
    --output_dir "deepseek-sft-lora"
    --per_device_train_batch_size 8
    --per_device_eval_batch_size 8
    --gradient_accumulation_steps 4
    --gradient_checkpointing True
    --use_reentrant True
    --dataset_text_field "content"
    --use_peft_lora True
    --lora_r 16
    --lora_alpha 16
    --lora_dropout 0.05
    --lora_target_modules "all-linear"
    --use_4bit_quantization True
    --use_nested_quant True
    --bnb_4bit_compute_dtype "bfloat16"

resources:
  # Consumes ~25GB of VRAM for QLoRA fine-tuning deepseek-ai/DeepSeek-V2-Lite
  gpu: 48GB
