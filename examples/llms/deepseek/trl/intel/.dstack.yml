type: task
# The name is optional, if not specified, generated randomly
name: trl-train

image: vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0

# Required environment variables
env:
  - MODEL_ID=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
  - WANDB_API_KEY
  - WANDB_PROJECT
# Commands of the task
commands:
  - pip install --upgrade-strategy eager optimum[habana]
  - pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.19.0
  - git clone https://github.com/huggingface/optimum-habana.git
  - cd optimum-habana/examples/trl
  - pip install -r requirements.txt
  - pip install wandb
  - DEEPSPEED_HPU_ZERO3_SYNC_MARK_STEP_REQUIRED=1 python ../gaudi_spawn.py --world_size 8 --use_deepspeed sft.py
    --model_name_or_path $MODEL_ID
    --dataset_name "lvwerra/stack-exchange-paired"
    --deepspeed ../language-modeling/llama2_ds_zero3_config.json
    --output_dir="./sft"
    --do_train
    --max_steps=500
    --logging_steps=10
    --save_steps=100
    --per_device_train_batch_size=1
    --per_device_eval_batch_size=1
    --gradient_accumulation_steps=2
    --learning_rate=1e-4
    --lr_scheduler_type="cosine"
    --warmup_steps=100
    --weight_decay=0.05
    --optim="paged_adamw_32bit"
    --lora_target_modules "q_proj" "v_proj"
    --bf16
    --remove_unused_columns=False
    --run_name="sft_deepseek_70"
    --report_to="wandb"
    --use_habana
    --use_lazy_mode

resources:
  gpu: gaudi2:8
