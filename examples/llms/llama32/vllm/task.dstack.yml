type: task
name: llama32-task-vllm

# If `image` is not specified, dstack uses its default image
python: "3.10"

# Required environment variables
env:
  - HUGGING_FACE_HUB_TOKEN
  - MODEL_ID=meta-llama/Llama-3.2-11B-Vision-Instruct
  - MAX_MODEL_LEN=13488
  - MAX_NUM_SEQS=40

commands:
  - pip install vllm
  - vllm serve $MODEL_ID
    --tensor-parallel-size $DSTACK_GPUS_NUM
    --max-model-len $MAX_MODEL_LEN
    --max-num-seqs $MAX_NUM_SEQS
    --enforce-eager
    --disable-log-requests
    --limit-mm-per-prompt "image=1"
# Expose the vllm server port
ports:
  - 8000

# Use either spot or on-demand instances
spot_policy: auto
# Uncomment to ensure it doesn't create a new fleet
#creation_policy: reuse

resources:
  # Required resources
  gpu: 48GB