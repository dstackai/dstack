type: service
name: llama31

image: ollama/ollama
commands:
  - ollama serve &
  - sleep 3
  - ollama pull llama3.1
  - fg
port: 11434
model: llama3.1

# Use either spot or on-demand instances
spot_policy: auto

# Required resources
resources:
  gpu: 24GB