type: task
name: llama31-task

# If `image` is not specified, dstack uses its default image
python: "3.10"

# Required environment variables
env:
  - HUGGING_FACE_HUB_TOKEN
commands:
  - pip install vllm==0.5.3.post1
  - vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --max-model-len 4096
# Expose the vllm server port
ports:
  - 8000

# Use either spot or on-demand instances
spot_policy: auto
# Uncomment to ensure it doesn't create a new fleet
#creation_policy: reuse

resources:
  # Required resources
  gpu: 24GB